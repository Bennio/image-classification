
# First we copy the values of values.yaml in variable to make it easier to access them
{{- $lrlist := .Values.hyperParamValues.learning_rate -}}
{{- $batchsizelist := .Values.hyperParamValues.train_batch_size -}}
{{- $image := .Values.container.image -}}
{{- $name := .Values.tfjob.name -}}
{{- $useGPU := .Values.tfjob.useGPU -}}
{{- $chartname := .Chart.Name -}}
{{- $chartversion := .Chart.Version -}}

# Then we loop over every value of $lrlist (learning rate) and $batchsize (train batch size)
# This will result in create 1 TFJob for every pair of learning rate and train batch size
{{- range $i, $lr := $lrlist }}
{{- range $j, $batchsize := $batchsizelist }}
apiVersion: kubeflow.org/v1alpha2
kind: TFJob # Each one of our trainings will be a separate TFJob
metadata:
  name: {{ $name }}-{{ $i }}-{{ $j }} # We give a unique name to each training
  labels:
    chart: "{{ $chartname }}-{{ $chartversion | replace "+" "_" }}"
spec: 
  tfReplicaSpecs:
    MASTER:
      template:
        spec:
          containers:
            - name: tensorflow
              image: {{ $image }}
              args:
                # Here we pass a unique learning rate and batch size to each instance.
                # We also put the values between quotes to avoid potential formatting issues    
                # We save the output in a different directory for each pod
                - "--bottleneck_dir=bottlenecks"
                - "--model_dir=/tf-output/image-retraining-lr{{ $lr }}-bs-{{ $batchsize }}/inception"
                - "--summaries_dir=/tf-output/training_summaries/image-retraining-lr{{ $lr }}-bs-{{ $batchsize }}/baseline"
                - "--output_graph=/tf-output/image-retraining-lr{{ $lr }}-bs-{{ $batchsize }}/retrained_graph.pb"
                - "--output_labels=/tf-output/image-retraining-lr{{ $lr }}-bs-{{ $batchsize }}/retrained_labels.txt"
                - "--image_dir=images"
                - "--learning_rate"
                - {{ $lr | quote }}
                - "--train_batch_size"
                - {{ $batchsize | quote }}
{{ if $useGPU }}  # We only want to request GPUs if we asked for it in values.yaml with useGPU
              resources:
                limits:
                  nvidia.com/gpu: 1
{{ end }}
              volumeMounts:
              - mountPath: /tf-output
                subPath: tfjob-hps # As usual we want to save everything in a separate subdirectory 
                name: {{.Values.pvcName}}
          volumes:
            - name: {{.Values.pvcName}}
              persistentVolumeClaim:
                claimName: {{.Values.pvcName}}
---
{{- end }}
{{- end }}
apiVersion: v1
kind: Service
metadata:
  name: tensorboard-hyperparam-sweep
  labels:
    name: tensorboard-hyperparam-sweep
spec:
  type: LoadBalancer
  ports:
  - name: http
    port: 80
    targetPort: 6006
  selector:
    app: tensorboard-hyperparam-sweep
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: tensorboard-hyperparam-sweep
  name: tensorboard-hyperparam-sweep
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tensorboard-hyperparam-sweep
  template:
    metadata:
      labels:
        app: tensorboard-hyperparam-sweep
    spec:
      volumes:
        - name: {{.Values.pvcName}}
          persistentVolumeClaim:
            claimName: {{.Values.pvcName}}
      containers:
        - name: tensorflow
          image: chzbrgr71/tensorboard:1.8
          args:                
            - "--logdir"
            - "/tf-output/training_summaries"
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /tf-output
              subPath: tfjob-hps
              name: {{.Values.pvcName}}
          ports:
            - containerPort: 6006
              protocol: TCP